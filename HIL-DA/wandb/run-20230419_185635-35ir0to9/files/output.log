
[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
[INFO] Connected to Unity environment with package version 2.3.0-exp.4 and communication version 1.5.0
[INFO] Connected new brain: Worm?team=0
[WARNING] The environment contains multiple observations. You must define allow_multiple_obs=True to receive them all. Otherwise, only the first visual observation (or vector observation ifthere are no visual observations) will be provided in the observation.
[WARNING] Could not seed environment Worm?team=0
D:\ProgramingEnvirment\python-3.7\lib\site-packages\gym\spaces\box.py:74: UserWarning: [33mWARN: Box bound precision lowered by casting to float32
  "Box bound precision lowered by casting to {}".format(self.dtype)
D:\ProgramingEnvirment\python-3.7\lib\site-packages\torch\nn\functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
e:/æ¨¡ä»¿å­¦ä¹ /ç ”ç©¶ç‚¹1/pofo-worm/2IWIL.py:317: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\utils\tensor_new.cpp:204.)
  expert_state_ns = torch.Tensor(expert_state_ns).to(device)
Episode 0	Average reward: 0.27	num_step: 1192.00	Loss (disc): 0.00
Episode 1	Average reward: 0.63	num_step: 1192.00	Loss (disc): 0.00
Episode 2	Average reward: 0.09	num_step: 1192.00	Loss (disc): 0.00
Episode 3	Average reward: 0.18	num_step: 1192.00	Loss (disc): 0.00
Episode 4	Average reward: 0.17	num_step: 1192.00	Loss (disc): 0.00
Episode 5	Average reward: 0.38	num_step: 1192.00	Loss (disc): 0.00
Episode 6	Average reward: 0.13	num_step: 1192.00	Loss (disc): 0.00
Episode 7	Average reward: 0.24	num_step: 1192.00	Loss (disc): 0.00
Episode 8	Average reward: 0.56	num_step: 1192.00	Loss (disc): 0.00
Episode 9	Average reward: 0.13	num_step: 1192.00	Loss (disc): 0.00
Episode 10	Average reward: 0.02	num_step: 1192.00	Loss (disc): 0.00
Episode 11	Average reward: 0.09	num_step: 1192.00	Loss (disc): 0.00
Episode 12	Average reward: 0.22	num_step: 1192.00	Loss (disc): 0.00
Episode 13	Average reward: 1.24	num_step: 1192.00	Loss (disc): 0.00
Episode 14	Average reward: 0.77	num_step: 1192.00	Loss (disc): 0.00
Episode 15	Average reward: 0.23	num_step: 1192.00	Loss (disc): 0.00
Episode 16	Average reward: 0.72	num_step: 1192.00	Loss (disc): 0.00
Episode 17	Average reward: 0.44	num_step: 1192.00	Loss (disc): 0.00
Episode 18	Average reward: 0.11	num_step: 1192.00	Loss (disc): 0.00
Episode 19	Average reward: 0.17	num_step: 1192.00	Loss (disc): 0.00
Episode 20	Average reward: 0.32	num_step: 1192.00	Loss (disc): 0.00
Episode 21	Average reward: 0.66	num_step: 1192.00	Loss (disc): 0.00
Episode 22	Average reward: 0.69	num_step: 1192.00	Loss (disc): 0.00
Episode 23	Average reward: 0.91	num_step: 1192.00	Loss (disc): 0.00
Episode 24	Average reward: 0.49	num_step: 1192.00	Loss (disc): 0.00
Episode 25	Average reward: 1.17	num_step: 1192.00	Loss (disc): 0.00
Episode 26	Average reward: 0.55	num_step: 1192.00	Loss (disc): 0.00
Episode 27	Average reward: 0.22	num_step: 1192.00	Loss (disc): 0.00
Episode 28	Average reward: 0.65	num_step: 1192.00	Loss (disc): 0.00
Episode 29	Average reward: 1.06	num_step: 1192.00	Loss (disc): 0.00
Episode 30	Average reward: 0.43	num_step: 1192.00	Loss (disc): 0.00
Episode 31	Average reward: 1.22	num_step: 1192.00	Loss (disc): 0.00
Episode 32	Average reward: 0.52	num_step: 1192.00	Loss (disc): 0.00
Episode 33	Average reward: 0.25	num_step: 1192.00	Loss (disc): 0.00
Episode 34	Average reward: 0.35	num_step: 1192.00	Loss (disc): 0.00
Episode 35	Average reward: 1.29	num_step: 1192.00	Loss (disc): 0.00
Episode 36	Average reward: 0.10	num_step: 1192.00	Loss (disc): 0.00
Episode 37	Average reward: 0.55	num_step: 1192.00	Loss (disc): 0.00
Episode 38	Average reward: 1.01	num_step: 1192.00	Loss (disc): 0.00
Episode 39	Average reward: 0.43	num_step: 1192.00	Loss (disc): 0.00
Episode 40	Average reward: 0.29	num_step: 1192.00	Loss (disc): 0.00
Episode 41	Average reward: 1.27	num_step: 1192.00	Loss (disc): 0.00
Episode 42	Average reward: 0.70	num_step: 1192.00	Loss (disc): 0.00
Episode 43	Average reward: 0.15	num_step: 1192.00	Loss (disc): 0.00
Episode 44	Average reward: 1.11	num_step: 1192.00	Loss (disc): 0.00
Episode 45	Average reward: 0.00	num_step: 1192.00	Loss (disc): 0.00
Episode 46	Average reward: 0.19	num_step: 1192.00	Loss (disc): 0.00
Episode 47	Average reward: 0.00	num_step: 1192.00	Loss (disc): 0.00
Episode 48	Average reward: 0.48	num_step: 1192.00	Loss (disc): 0.00
Episode 49	Average reward: 0.73	num_step: 1192.00	Loss (disc): 0.00
Episode 50	Average reward: 0.25	num_step: 1192.00	Loss (disc): 0.00
Episode 51	Average reward: 0.00	num_step: 1192.00	Loss (disc): 0.00
Episode 52	Average reward: 0.76	num_step: 1192.00	Loss (disc): 0.00
Traceback (most recent call last):
  File "e:/æ¨¡ä»¿å­¦ä¹ /ç ”ç©¶ç‚¹1/pofo-worm/2IWIL.py", line 269, in <module>
    next_state, true_reward, done, _ = env.step(action)
  File "D:\ProgramingEnvirment\python-3.7\lib\site-packages\gym_unity\envs\__init__.py", line 201, in step
    self._env.step()
  File "D:\ProgramingEnvirment\python-3.7\lib\site-packages\mlagents_envs\timers.py", line 305, in wrapped
    return func(*args, **kwargs)
  File "D:\ProgramingEnvirment\python-3.7\lib\site-packages\mlagents_envs\environment.py", line 350, in step
    raise UnityCommunicatorStoppedException("Communicator has exited.")
mlagents_envs.exception.UnityCommunicatorStoppedException: Communicator has exited.